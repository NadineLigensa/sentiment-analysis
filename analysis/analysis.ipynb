{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ideas which model to use:\n",
    "## https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
    "## https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset for training\n",
    "##https://huggingface.co/datasets/mteb/tweet_sentiment_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import emoji dataset from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"arbml/emoji_sentiment_lexicon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dataset to pd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugriff auf den Trainingsdatensatz\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Konvertieren Sie den Trainingsdatensatz in ein Pandas DataFrame\n",
    "df = pd.DataFrame(train_dataset)\n",
    "\n",
    "df.to_csv('../data/emoji_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m15\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Converting emojis to raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GARBAGE COMPANY üóëüóëüóë', 'I love this  Smiling Face With Heart-Eyes  Red Heart Ô∏è Raising Hands ', 'This shit is another level  Red Heart Ô∏è']\n"
     ]
    }
   ],
   "source": [
    "# test = [\"GARBAGE COMPANY üóëüóëüóë\", \"I love this üòç‚ù§Ô∏èüôå\", \"This shit is another level ‚ù§Ô∏è\"]\n",
    "# # Erstellen einer Funktion, um Emojis in einem Text durch ihre Unicode-Namen zu ersetzen\n",
    "# def ersetze_emojis(text):\n",
    "#     for index, row in df.iterrows():\n",
    "#         emoji = row['Emoji']\n",
    "#         unicode_name = row['Unicode_Name']\n",
    "#         text = text.replace(emoji, unicode_name)\n",
    "#     return text\n",
    "\n",
    "# bereinigte_kommentare = [ersetze_emojis(kommentar) for kommentar in test]\n",
    "\n",
    "# print(bereinigte_kommentare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text of emojis and insert raw text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load emoji data from data/... .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden Sie den Datensatz mit den Unicode-Namen der Emojis\n",
    "emoji_data = {}\n",
    "with open('../data/emoji_dataset.csv', mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        emoji_data[row[2]] = row[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process texts and substitute emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ersetze_emojis(text):\n",
    "    for emoji, name in emoji_data.items():\n",
    "        text = text.replace(emoji, name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(filepath):\n",
    "    df_original = pd.read_csv(filepath)\n",
    "    df_bereinigt = df_original.copy()  \n",
    "    df_bereinigt['Text'] = df_bereinigt['Text'].apply(ersetze_emojis)\n",
    "\n",
    "    return df_bereinigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bereinige text mit emojis\n",
    "dataframe_bereinigt = clean_text(\"../files/oatly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Is_Activism</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post_Id</th>\n",
       "      <th>Like_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm tired of seeing brands and influencers was...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CvfAOApNdUM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>It‚Äôs driving me crazy that the triangle ‚ÄòA‚Äô lo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CvfAOApNdUM</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>omfg</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CvfAOApNdUM</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This sound scraches my brain. I like it  Glass...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CvfAOApNdUM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is so cool and oddly soothing! We love oa...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CvfAOApNdUM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>704</td>\n",
       "      <td>As much as I like oatmilk more than cow milk. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>CsGiGN0R_Y9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>705</td>\n",
       "      <td>Red Heart Ô∏è It</td>\n",
       "      <td>False</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>CsGiGN0R_Y9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>706</td>\n",
       "      <td>or actually cost to produce/cost to buy!!  Fac...</td>\n",
       "      <td>False</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>CsGiGN0R_Y9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>707</td>\n",
       "      <td>Raising Hands</td>\n",
       "      <td>False</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>CsGiGN0R_Y9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>708</td>\n",
       "      <td>Clapping Hands  Clapping Hands  Clapping Hands</td>\n",
       "      <td>False</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>CsGiGN0R_Y9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>708 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               Text  Is_Activism  \\\n",
       "0      1  I'm tired of seeing brands and influencers was...        False   \n",
       "1      2  It‚Äôs driving me crazy that the triangle ‚ÄòA‚Äô lo...        False   \n",
       "2      3                                               omfg        False   \n",
       "3      4  This sound scraches my brain. I like it  Glass...        False   \n",
       "4      5  This is so cool and oddly soothing! We love oa...        False   \n",
       "..   ...                                                ...          ...   \n",
       "703  704  As much as I like oatmilk more than cow milk. ...        False   \n",
       "704  705                                     Red Heart Ô∏è It        False   \n",
       "705  706  or actually cost to produce/cost to buy!!  Fac...        False   \n",
       "706  707                                     Raising Hands         False   \n",
       "707  708    Clapping Hands  Clapping Hands  Clapping Hands         False   \n",
       "\n",
       "          Category      Post_Id  Like_Count  \n",
       "0              NaN  CvfAOApNdUM           1  \n",
       "1              NaN  CvfAOApNdUM           8  \n",
       "2              NaN  CvfAOApNdUM           2  \n",
       "3              NaN  CvfAOApNdUM           0  \n",
       "4              NaN  CvfAOApNdUM           0  \n",
       "..             ...          ...         ...  \n",
       "703  Environmental  CsGiGN0R_Y9           0  \n",
       "704  Environmental  CsGiGN0R_Y9           0  \n",
       "705  Environmental  CsGiGN0R_Y9           0  \n",
       "706  Environmental  CsGiGN0R_Y9           0  \n",
       "707  Environmental  CsGiGN0R_Y9           0  \n",
       "\n",
       "[708 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_bereinigt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing with first model (cardiffnlp/twitter-xlm-roberta-base-sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5927, 0.5257, 0.8787],\n",
      "        [0.9849, 0.0781, 0.0509],\n",
      "        [0.9503, 0.9423, 0.3538],\n",
      "        [0.9962, 0.0812, 0.4980],\n",
      "        [0.8752, 0.0305, 0.4194]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich hasse dieses produkt, bitte nie mehr werbung daf√ºr machen\n",
      "1) neutral 0.387\n",
      "2) negative 0.3623\n",
      "3) positive 0.2507\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Ich hasse dieses produkt, bitte nie mehr werbung daf√ºr machen\"\n",
    "text = preprocess(text)\n",
    "print(text)\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "\n",
    "output = model(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.9490166902542114}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
    "result = sentiment_task(\"I hate this company!\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.9490166902542114}]\n"
     ]
    }
   ],
   "source": [
    "result = sentiment_task(\"I hate this company!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on different data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.9064\n",
      "2) neutral 0.0601\n",
      "3) positive 0.0335\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "# Speichere den Tokenizer an einem anderen Ort\n",
    "tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on whatever string you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.7673\n",
      "2) neutral 0.2015\n",
      "3) negative 0.0313\n"
     ]
    }
   ],
   "source": [
    "text = \"Good night üòä\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night üòä\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Test Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# URL zur JSON-Datenquelle\n",
    "url = \"https://datasets-server.huggingface.co/rows?dataset=tweet_eval&config=sentiment&split=validation&offset=0&limit=100\"\n",
    "\n",
    "# Die Anfrage an die URL senden\n",
    "response = requests.get(url)\n",
    "\n",
    "# √úberpr√ºfen, ob die Anfrage erfolgreich war (Statuscode 200)\n",
    "if response.status_code == 200:\n",
    "    # JSON-Daten aus der Antwort extrahieren\n",
    "    json_data = response.json()\n",
    "    \n",
    "    # Erstelle ein leeres Pandas DataFrame mit den gew√ºnschten Spalten 'text' und 'label'\n",
    "    df = pd.DataFrame(columns=['text', 'label'])\n",
    "    \n",
    "    # Schleife durch die Zeilen in den JSON-Daten und f√ºge sie dem DataFrame hinzu\n",
    "    for row in json_data['rows']:\n",
    "        text = row['row']['text']\n",
    "        label = row['row']['label']\n",
    "        df = pd.concat([df, pd.DataFrame({'text': [text], 'label': [label]})], ignore_index=True)\n",
    "    \n",
    "    # Jetzt enth√§lt der DataFrame alle Zeilen aus den JSON-Daten\n",
    "    # Du kannst diesen DataFrame verwenden oder ihn in eine Datei speichern\n",
    "    # Zum Beispiel, um ihn als CSV-Datei zu speichern:\n",
    "    #df.to_csv(\"dataset.csv\", index=False)\n",
    "else:\n",
    "    print(\"Fehler beim Abrufen der Daten:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dark Souls 3 April Launch Date Confirmed With ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"National hot dog day, national tequila day, t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When girls become bandwagon fans of the Packer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user I may or may not have searched it up on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here's your starting TUESDAY MORNING Line up a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>I hope u all have a good day bc it's Friday an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>\"I'm still not over Kendrick's \"\"Black Friday\"...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>@user  Welcome to the 21st century, cuz!  Ging...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>@user Dear Taimouraga, Thank you for contactin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>omg then I sat on my floor in front of the TV ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label\n",
       "0   Dark Souls 3 April Launch Date Confirmed With ...     1\n",
       "1   \"National hot dog day, national tequila day, t...     2\n",
       "2   When girls become bandwagon fans of the Packer...     0\n",
       "3   @user I may or may not have searched it up on ...     1\n",
       "4   Here's your starting TUESDAY MORNING Line up a...     1\n",
       "..                                                ...   ...\n",
       "95  I hope u all have a good day bc it's Friday an...     2\n",
       "96  \"I'm still not over Kendrick's \"\"Black Friday\"...     2\n",
       "97  @user  Welcome to the 21st century, cuz!  Ging...     2\n",
       "98  @user Dear Taimouraga, Thank you for contactin...     2\n",
       "99  omg then I sat on my floor in front of the TV ...     2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Anfangs-Offset und Schrittgr√∂√üe f√ºr die Abfrage\n",
    "start_offset = 0\n",
    "step = 100  # Die Anzahl der Eintr√§ge, die pro Schritt abgerufen werden sollen\n",
    "\n",
    "# URL zur JSON-Datenquelle\n",
    "url_template = \"https://datasets-server.huggingface.co/rows?dataset=tweet_eval&config=sentiment&split=validation&offset={offset}&limit={limit}\"\n",
    "\n",
    "# Erstelle ein leeres Pandas DataFrame mit den gew√ºnschten Spalten 'text' und 'label'\n",
    "df = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "while True:\n",
    "    # Konstruiere die URL mit dem aktuellen Offset\n",
    "    url = url_template.format(offset=start_offset, limit=step)\n",
    "\n",
    "    # Die Anfrage an die URL senden\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # √úberpr√ºfen, ob die Anfrage erfolgreich war (Statuscode 200)\n",
    "    if response.status_code == 200:\n",
    "        # JSON-Daten aus der Antwort extrahieren\n",
    "        json_data = response.json()\n",
    "\n",
    "        # Schleife durch die Zeilen in den JSON-Daten und f√ºge sie dem DataFrame hinzu\n",
    "        for row in json_data['rows']:\n",
    "            text = row['row']['text']\n",
    "            label = row['row']['label']\n",
    "            df = pd.concat([df, pd.DataFrame({'text': [text], 'label': [label]})], ignore_index=True)\n",
    "\n",
    "        # Wenn weniger Eintr√§ge abgerufen wurden als angefordert, brechen Sie die Schleife ab\n",
    "        if len(json_data['rows']) < step:\n",
    "            break\n",
    "\n",
    "        # Erh√∂he den Offset f√ºr den n√§chsten Schritt\n",
    "        start_offset += step\n",
    "    else:\n",
    "        print(\"Fehler beim Abrufen der Daten:\", response.status_code)\n",
    "        break\n",
    "\n",
    "# Jetzt enth√§lt der DataFrame alle Zeilen aus den JSON-Daten\n",
    "# Sie k√∂nnen diesen DataFrame verwenden oder ihn in eine Datei speichern\n",
    "# Zum Beispiel, um ihn als CSV-Datei zu speichern:\n",
    "# df.to_csv(\"dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dark Souls 3 April Launch Date Confirmed With ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"National hot dog day, national tequila day, t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When girls become bandwagon fans of the Packer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user I may or may not have searched it up on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here's your starting TUESDAY MORNING Line up a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>\"LONDON (AP) \"\" Prince George celebrates his s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Harper's Worst Offense against Refugees may be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Hold on... Sam Smith may do the theme to Spect...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Gonna watch Final Destination 5 tonight. I alw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>\"Interview with Devon Alexander \\\"\"\"\"Speed Kil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Dark Souls 3 April Launch Date Confirmed With ...     1\n",
       "1     \"National hot dog day, national tequila day, t...     2\n",
       "2     When girls become bandwagon fans of the Packer...     0\n",
       "3     @user I may or may not have searched it up on ...     1\n",
       "4     Here's your starting TUESDAY MORNING Line up a...     1\n",
       "...                                                 ...   ...\n",
       "1995  \"LONDON (AP) \"\" Prince George celebrates his s...     1\n",
       "1996  Harper's Worst Offense against Refugees may be...     1\n",
       "1997  Hold on... Sam Smith may do the theme to Spect...     2\n",
       "1998  Gonna watch Final Destination 5 tonight. I alw...     1\n",
       "1999  \"Interview with Devon Alexander \\\"\"\"\"Speed Kil...     1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/val_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe54a73c18050b6d50975a0cc591f481f480ecb39df2bfc4b76ac59282f6b0b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
