{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try this model: https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sulu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sulu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji==0.6.0 in /Users/sulu/opt/anaconda3/lib/python3.9/site-packages (0.6.0)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip3 install emoji==0.6.0\n",
    "MODEL = f\"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rb/6r0tm6px2yg64kgqwlg_q0800000gn/T/ipykernel_22264/348798047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0memoji_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/emoji_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0memoji_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Laden Sie den Datensatz mit den Unicode-Namen der Emojis\n",
    "emoji_data = {}\n",
    "with open('../../data/emoji_dataset.csv', mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        emoji_data[row[2]] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ersetze_emojis(text):\n",
    "    for emoji, name in emoji_data.items():\n",
    "        text = text.replace(emoji, name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on weirdo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòç‚ù§Ô∏èüî•\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rb/6r0tm6px2yg64kgqwlg_q0800000gn/T/ipykernel_22264/1121902118.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"üòç‚ù§Ô∏èüî•\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genauigkeit: 83.60%\n"
     ]
    }
   ],
   "source": [
    "# Laden Sie Ihren CSV-Datensatz\n",
    "csv_file = \"../../data/val_data.csv\"  # Ersetzen Sie durch den Pfad zu Ihrer CSV-Datei\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Leere Listen zum Speichern der vorhergesagten Sentiments und der tats√§chlichen Labels\n",
    "predicted_sentiments = []\n",
    "actual_labels = []\n",
    "\n",
    "# Durchlaufen Sie jeden Eintrag im DataFrame und wenden Sie das Modell an\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    #text = preprocess(text)  # Stellen Sie sicher, dass Sie Ihre Preprocessing-Funktion hier verwenden\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    softmax_scores = softmax(scores)\n",
    "\n",
    "    # Extrahieren Sie das vorhergesagte Sentiment\n",
    "    predicted_label_id = np.argmax(softmax_scores)\n",
    "    #predicted_sentiment = model.config.id2label[predicted_label_id]\n",
    "\n",
    "    # Speichern Sie das vorhergesagte Sentiment und das tats√§chliche Label\n",
    "    predicted_sentiments.append(predicted_label_id)\n",
    "    actual_labels.append(row['label'])\n",
    "\n",
    "# F√ºgen Sie die vorhergesagten Sentiments und die tats√§chlichen Labels als neue Spalten zum DataFrame hinzu\n",
    "df['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Vergleichen Sie die vorhergesagten Sentiments mit den tats√§chlichen Labels\n",
    "correct_predictions = (df['predicted_sentiment'] == df['label']).sum()\n",
    "total_predictions = len(df)\n",
    "\n",
    "# Berechnen Sie die Genauigkeit (Accuracy) der Vorhersagen\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Drucken Sie die Genauigkeit\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dark Souls 3 April Launch Date Confirmed With ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"National hot dog day, national tequila day, t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When girls become bandwagon fans of the Packer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user I may or may not have searched it up on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here's your starting TUESDAY MORNING Line up a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Dark Souls 3 April Launch Date Confirmed With ...      1\n",
       "1  \"National hot dog day, national tequila day, t...      2\n",
       "2  When girls become bandwagon fans of the Packer...      0\n",
       "3  @user I may or may not have searched it up on ...      1\n",
       "4  Here's your starting TUESDAY MORNING Line up a...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genauigkeit: 71.90%\n"
     ]
    }
   ],
   "source": [
    "# Laden Sie Ihren CSV-Datensatz\n",
    "csv_file = \"../../data/test_data.csv\"  # Ersetzen Sie durch den Pfad zu Ihrer CSV-Datei\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Leere Listen zum Speichern der vorhergesagten Sentiments und der tats√§chlichen Labels\n",
    "predicted_sentiments = []\n",
    "actual_labels = []\n",
    "\n",
    "# Durchlaufen Sie jeden Eintrag im DataFrame und wenden Sie das Modell an\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    #text = preprocess(text)  # Stellen Sie sicher, dass Sie Ihre Preprocessing-Funktion hier verwenden\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    softmax_scores = softmax(scores)\n",
    "\n",
    "    # Extrahieren Sie das vorhergesagte Sentiment\n",
    "    predicted_label_id = np.argmax(softmax_scores)\n",
    "    #predicted_sentiment = model.config.id2label[predicted_label_id]\n",
    "\n",
    "    # Speichern Sie das vorhergesagte Sentiment und das tats√§chliche Label\n",
    "    predicted_sentiments.append(predicted_label_id)\n",
    "    actual_labels.append(row['label'])\n",
    "\n",
    "# F√ºgen Sie die vorhergesagten Sentiments und die tats√§chlichen Labels als neue Spalten zum DataFrame hinzu\n",
    "df['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Vergleichen Sie die vorhergesagten Sentiments mit den tats√§chlichen Labels\n",
    "correct_predictions = (df['predicted_sentiment'] == df['label']).sum()\n",
    "total_predictions = len(df)\n",
    "\n",
    "# Berechnen Sie die Genauigkeit (Accuracy) der Vorhersagen\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Drucken Sie die Genauigkeit\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print confusion matrix and statistics for the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# Berechnen Sie die Verwirrungsmatrix\n",
    "conf_matrix = confusion_matrix(df['label'], df['predicted_sentiment'])\n",
    "\n",
    "# Drucken Sie die Verwirrungsmatrix\n",
    "print(\"Verwirrungsmatrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Berechnen Sie die Precision und den Recall f√ºr jede Klasse\n",
    "precision = precision_score(df['label'], df['predicted_sentiment'], average=None)\n",
    "recall = recall_score(df['label'], df['predicted_sentiment'], average=None)\n",
    "\n",
    "# Drucken Sie die Precision und den Recall f√ºr jede Klasse\n",
    "print(\"Precision pro Klasse:\", precision)\n",
    "print(\"Recall pro Klasse:\", recall)\n",
    "\n",
    "# Berechnen Sie den gewichteten Durchschnitt von Precision und Recall (Macro-Durchschnitt)\n",
    "macro_precision = precision_score(df['label'], df['predicted_sentiment'], average='macro')\n",
    "macro_recall = recall_score(df['label'], df['predicted_sentiment'], average='macro')\n",
    "\n",
    "# Drucken Sie den gewichteten Durchschnitt von Precision und Recall\n",
    "print(f\"Macro Precision: {macro_precision:.2f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe54a73c18050b6d50975a0cc591f481f480ecb39df2bfc4b76ac59282f6b0b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
